\documentclass{report}
% Smaller paper size for easier reading, print two pages per a4 side.
\usepackage[a5paper]{geometry}
\usepackage{pdflscape}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{calc}
\usepackage[T1]{fontenc}
\setlist[description]{leftmargin=\parindent,labelindent=\parindent}%Indent descriptions
\MakeOuterQuote{"}
% Indent first paragraphs in sections and chapters
\usepackage{indentfirst}
\title{Bioinformatics Notes}
\author{Artem Vasenin}
\date{Last updated \today}

% Make quotes smaller than normal text
\expandafter\def\expandafter\quote\expandafter{\quote\small\singlespacing}


% Keep each sentense on separate line to help with version control.
% Quotes can remain on one line since they don't usually change.

\begin{document}

\maketitle
\cleardoublepage
\pagenumbering{gobble}
\tableofcontents
\cleardoublepage
\pagenumbering{arabic}

\chapter{Biology background}
\paragraph{Structure of DNA}
DNA is structured as a double helix.
Each helix is a sequence of nucleotides, there are four types of nucleotides: A,C,T and G.
The sequences are aligned with each other such that at each position the nucleotide on one helix is paired with nucleotide on the other helix in one of the four possibilities: A-T, T-A, C-G or G-C.
\paragraph{Structure of proteins}
DNA uses a triplet code, triplet of nucleotides is called a "codon".
Each codon is a code for a specific amino acid.
When multiple amino acids join together they form a protein.
There are 20 types of amino acids which can be used to form a protein.
Some codons are used for punctuation.
A gene is a sequence of codons between start and stop codons.
Each gene is responsible for one protein.
\paragraph{DNA - Protein translation}
To copy DNA, it is first untwisted and unzipped inside the nucleus.
Then free RNA nucleotides form complementary base pairs with one of the DNA strands.
RNA uses a U nucleotide instead of DNA's T, so they can be paired as follows: A-U, T-A, C-G and G-C; DNA on the left, RNA on the right.
When multiple RNA nucleotides bond next to each other, weak hydrogen bons form between those nucleotides.
mRNA transcribes amino acid code using codons.
mRNA strand is then synthesised from the sequence of RNA nucleotides.
mRNA strand peels off the DNA and moves out of the nucleus into the cytoplasm.

Translation takes place on the ribosome in the cytoplasm.
The ribosomes are sites of protein synthesis.
mRNA strand attaches to a ribosome and tRNA molecules transport specific amino acids to the ribosome.
The anti-codons and codons match up to form complementary base pairs.
Peptide bonds form between the adjacent amino acids to form the polypeptide, which is a protein.

\chapter{Sequence Alignment}
The goal of the alignment algorithms is to find the longest common subsequence of two sequences.
Alignment of $n$ sequences is an $n\text{-row}$ matrix, where $n$th row represents the symbols of $n$th sequence (in order) interspersed by "-".
Corresponding symbols in different sequences can have four different relations to each other:
\begin{description}
\item[Match] The symbols are the same.
\item[Mismatch] The symbols are different.
\item[Insertion] "Primary" symbol is "-".
\item[Deletion] "Secondary" symbol is "-".
\end{description}
Matches in alignment of two sequences form "common subsequence".
\paragraph{Global Alignment}
We can represent the alignment search space as an edge-weighted directed acyclic graph.
We can construct such DAC by creating a rectangular grid of nodes, with source in the top-left and destination in the bottom-right.
Then we add edges, of which there are three types:
\begin{description}
\item[Match/Mismatch] Connects neighbouring nodes diagonally\break from top-left to bottom-right.
\item[Insertion] Connects neighbouring nodes horizontally from left to right.
\item[Deletion] Connects neighbouring nodes vertically from top to bottom.
\end{description}
Global alignment can be found by finding the longest path from the source to the destination.
To find the longest path we start at the source and calculate the length of path to all its children.
To do that we consider all incoming edges and their source nodes.
The longest path will go through the edge which has the greatest sum of its cost and path cost to its source node.
Once we calculate which edge the longest path goes through that's the only information we need to store.
Therefore we can iteratively calculate path length to all nodes.
Finally to calculate the longest path from source to destination we start at the destination and iteratively move backwards along the stored edges until we arrive at the source.
\paragraph{Parametrisation}
We can parametrise the longest path by assigning different weights to different types of edges.
Match/mismatch edges can be weighted by considering how likely it is for the codon in primary sequence to mutate into codon in the secondary sequence.
These weights are usually calculated empirically.
Insertions and deletions on the other hand depend only on their length.
Combined insertions/deletion should usually have lower penalty that separate ones.
There are two costs: cost to begin insertion/deletion and cost to extend it.
Our DAC can be extended to represent that by adding vertical and horizontal edges that go across multiple nodes.
\paragraph{Local alignment}
Local alignment refers to a long sequence of uninterrupted matches.
This corresponds to the idea that two sequences should not align perfectly, but rather only their ends should align.
We can compute local alignment by computing global alignment in sub-rectangle of the graph.
To do this we add a zero-weight edge between top-left node in the sub-rectangle and source and another between bottom-right node and destination.
\paragraph{Improving space complexity}
Since DNA sequences are very long, we frequently run of working space.
Finding the longest path has $O(nm)$ space complexity where $n$ and $m$ are lengths of sequences to align.
We can improve space complexity to $O(n)$ by sacrificing time.
This is possible since to we can compute the middle node using only $O(n)$ space by considering each column in turn.
Since the results of column $i$ only rely on the results of column $i-1$, we only need to store at most two columns in memory.
If we compute scores for middle column coming from both sides, we can find the middle node.
After we found the middle node, we divide the graph into four rectangles and then recur on top-left and bottom-right rectangles.
By doing that we improve space complexity to $O(n)$, since we only ever finding the middle node, while keeping time complexity the same, but increasing it by a constant.
\paragraph{BLAST}
BLAST stands for Basic Local Alignment Search Tool.
It is a program designed for comparing protein sequences against a large database to find local similarity.
It uses a hash table to store all possible sub-sequences of a given length in the database.
BLAST relies on assumption that we only care for near perfect alignments >95\%, therefore we expect to see long runs of perfectly matching sub-sequences.
It uses a heuristic to find short matches between sequences and then uses those to start the alignment.
%"Introduce wild-cards to allow near-perfect matches" - not sure what that means.
\chapter{Clustering}
Clustering combines elements that are close to each other into the same cluster.
Since we expect similar sequences to have the same ancestor, we can use clustering to build the evolutionary tree.
\section{Types of clustering algorithms}
There are four main types of clustering algorithms:
\begin{itemize}
\item Hierarchical
\item k-means
\item Distribution based
\item Density based
\end{itemize}
\paragraph{Hierarchical clustering}
This approach clusters points based on the distance between them.
It works as follows:
\begin{enumerate}
\item Consider each node as a cluster
\item Calculate distances between all clusters
\item Find the smallest distance and combine corresponding clusters into one, using weighted average for new position.
\item Repeat previous two steps until there is only one cluster left.
\end{enumerate}
In parallel to merging clusters we create a leaf for every cluster at the start.
When two clusters are merged we create a new vertex and connect it to the vertices which represent the clusters which were combined.
After clustering is complete we can separate our set into $n$ clusters by undoing $n$ merges in reverse order.
\paragraph{k-means clustering}
This approach combines nodes into $k$ clusters, it works as follows:
\begin{enumerate}
\item Assign $k$ arbitrary centres.
\item Assign each node to a cluster based on the closest centre.
\item Calculate the centre of gravity of each cluster and move its centre to the node which is closest to the centre of gravity.
\item Repeat previous two steps until centres stop moving.
\end{enumerate}
Optimal number of clusters ($k$) has high cluster stability, e.g.\ it doesn't matter which nodes you choose as initial centres, final clusters will always be the same.
k-means is very fast as well.
\paragraph{Distribution based clustering} %TODO
\paragraph{Density based clustering} %TODO
\section{Clustering vs.\ Classification}
Clustering is unsupervised and should be used when we want to find if there are any relationships between the data points.
Classification on the other hand is supervised and should be used when we want to separate the data into predetermined classes.
\section{Distance and Similarity}
Distance and similarity refer to what percentage of symbols mismatch between two data-sets.
Distance metrics must obey triangle inequality.
E.g.\ if we have two biased coins and we flip them 100 times we can count how many times each coin landed heads up.
Based on how close the two counts are, we can infer that coins are either similarly weighted or not.

This is relevant because we can infer from how objects behave how similar they are.
Similar things usually are genetically closer to each other than distant ones.
Therefore we can construct evolution tree using similarities.

\chapter{Markov models}
\begin{description}
\item[Markov property] The future only depends on the present and not on the past.
\end{description}
Markov chain is a stochastic process that satisfies markov property.
Hidden markov model is a model which is assumed to be a markov chain with unobserved states.
\section{Markov-clustering (MCL)}
MCL assumes that dense cluster correspond to regions with a large number of paths.
It takes an undirected graph as an input and alternatively expands the matrix (multiplying it by itself) and inflating it (element wise multiplication).
It's a good algorithm to use if we don't know how many clusters exist in the data.
\end{document}
